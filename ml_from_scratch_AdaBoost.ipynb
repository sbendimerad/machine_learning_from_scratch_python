{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ML from scratch : AdaBoost, a step by step code with python"
      ],
      "metadata": {
        "id": "UZmw_qLlqyw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By Sabrine Bendimerad\n",
        "\n",
        "[Lien de l'article (FR)](https://medium.com/@sabrine.bendimerad1/adaboost-d%C3%A9cryptage-%C3%A9tape-par-%C3%A9tape-d53878335cdf)\n",
        "\n",
        "[Article link (EN)](https://)"
      ],
      "metadata": {
        "id": "3T6LFK7Uq5Bd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 : AdaBoost implementation"
      ],
      "metadata": {
        "id": "B7aWQhzSrUm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def decision_stump(X, y, weights):\n",
        "    \"\"\"\n",
        "    Finds the best decision stump using a single feature.\n",
        "    Parameters:\n",
        "    - X: numpy array of features\n",
        "    - y: numpy array of labels\n",
        "    - weights: numpy array of instance weights\n",
        "    Returns:\n",
        "    - best_stump: dictionary containing the best stump parameters\n",
        "    \"\"\"\n",
        "    num_features = X.shape[1]\n",
        "    best_stump = {'feature': None, 'threshold': None, 'polarity': None, 'error': float('inf'), 'predictions': None}\n",
        "\n",
        "    for feature in range(num_features):\n",
        "        feature_values = X[:, feature]\n",
        "        thresholds = np.unique(feature_values)\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            for polarity in [1, -1]:\n",
        "                predictions = np.ones(len(y))  # Initialize all predictions to 1.\n",
        "                # Apply threshold: if polarity is 1, predict -1 if feature value is below threshold.\n",
        "                if polarity == 1:\n",
        "                    predictions[X[:, feature] < threshold] = -1\n",
        "                else:  # If polarity is -1, predict 1 if feature value is above threshold.\n",
        "                    predictions[X[:, feature] > threshold] = 1\n",
        "\n",
        "                error = sum(weights[y != predictions])  # Calculate weighted error.\n",
        "\n",
        "                if error < best_stump['error']:  # Update best stump if current one has lower error.\n",
        "                    best_stump['error'] = error\n",
        "                    best_stump['feature'] = feature\n",
        "                    best_stump['threshold'] = threshold\n",
        "                    best_stump['polarity'] = polarity\n",
        "                    best_stump['predictions'] = predictions  # Store predictions in the stump.\n",
        "\n",
        "\n",
        "    return best_stump"
      ],
      "metadata": {
        "id": "ieMN6_ZRpUCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaBoost:\n",
        "    \"\"\"\n",
        "    AdaBoost algorithm implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_learners=5):\n",
        "        self.n_learners = n_learners  # Number of weak learners to use.\n",
        "        self.learners = []  # List of weak learners.\n",
        "        self.alphas = []  # List of learner weights.\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains the AdaBoost model.\n",
        "        Parameters:\n",
        "        - X: numpy array of features\n",
        "        - y: numpy array of labels\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        weights = np.full(n_samples, (1 / n_samples))  # Initialize weights equally.\n",
        "\n",
        "        for _ in range(self.n_learners):\n",
        "            stump = decision_stump(X, y, weights)\n",
        "            predictions = stump['predictions']  # Use predictions from the decision stump.\n",
        "\n",
        "\n",
        "            error = sum(weights[y != predictions])\n",
        "            alpha = 0.5 * np.log((1.0 - error) / (error + 1e-10))  # Calculate alpha.\n",
        "\n",
        "            weights *= np.exp(-alpha * y * predictions)  # Update weights.\n",
        "            weights /= np.sum(weights)  # Normalize weights.\n",
        "\n",
        "            self.learners.append(stump)  # Save the current stump.\n",
        "            self.alphas.append(alpha)  # Save the current alpha.\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Makes predictions using the trained AdaBoost model.\n",
        "        Parameters:\n",
        "        - X: numpy array of features to predict\n",
        "        Returns:\n",
        "        - final_predictions: numpy array of predicted labels\n",
        "        \"\"\"\n",
        "        final_predictions = np.zeros(X.shape[0])\n",
        "        for alpha, learner in zip(self.alphas, self.learners):\n",
        "            predictions = np.ones(X.shape[0])\n",
        "            if learner['polarity'] == 1:\n",
        "                predictions[X[:, learner['feature']] < learner['threshold']] = -1\n",
        "            else:\n",
        "                predictions[X[:, learner['feature']] > learner['threshold']] = 1\n",
        "            final_predictions += alpha * predictions\n",
        "\n",
        "        return np.sign(final_predictions)  # Return sign of predictions for classification."
      ],
      "metadata": {
        "id": "qY8TiN89g6uR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 : AdaBoost test"
      ],
      "metadata": {
        "id": "A8W1xxqLrY-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Assuming AdaBoost class is defined as previously discussed\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert labels to -1 and 1\n",
        "y[y == 0] = -1\n",
        "y[y == 1] = 1\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the AdaBoost model\n",
        "model = AdaBoost(n_learners=10)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcYH2QJprbKl",
        "outputId": "31ec8b82-5d92-4215-dd9a-80e80aa4e06f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.61\n"
          ]
        }
      ]
    }
  ]
}